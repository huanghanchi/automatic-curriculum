import torch


def train(model, encoder_optimizer, decoder_optimizer, criterion, inputs, labels):
    # inputs should be a tensor of shape (2 * seq_len + 1) x batch_size x  11 (e.g. generated by the data_generator)
    # labels should be a tensor of shape (seq_len + 1) x batch_size (e.g. generated by the data_generator)
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    decoder_output = model(inputs)

    loss = criterion(torch.transpose(torch.transpose(decoder_output, 0, 1), 1, 2), torch.transpose(labels, 0, 1))

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    loss_per_digit = loss.item() / labels.size(0)
    train_accuracy = get_accuracy(decoder_output, labels)

    return loss_per_digit, train_accuracy


def get_accuracy(predicted, labels):
    # predicted should be a tensor of shape (seq_len + 1) x batch_size x 10
    # labels should be a tensor of shape (seq_len + 1) x batch_size
    greedy_prediction = predicted.argmax(dim=2)

    per_digit_accuracy = (greedy_prediction == labels).float().mean().item()
    per_number_accuracy = (greedy_prediction == labels).float().min(dim=0)[0].mean().item()

    return per_digit_accuracy, per_number_accuracy
